{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# translating matlab code parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# paths\n",
    "lvm_path = Path(r'data/qim_20kbps_10db_l2_v2.lvm')\n",
    "\n",
    "# signal specific params\n",
    "spb = 20\n",
    "header = [0 if i % 2 == 0 else 1 for i in range(10)]\n",
    "print(f'header: {header}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParserLVM:\n",
    "    def __init__(self, lvm_path, spb, header=None):\n",
    "        print('initializing..')\n",
    "        \n",
    "        # set input path\n",
    "        if isinstance(lvm_path, Path):\n",
    "            try: \n",
    "                assert lvm_path.exists()\n",
    "                self.lvm_path = lvm_path\n",
    "                print(f'\\tset input lvm path: {self.lvm_path}')\n",
    "            except AssertionError:\n",
    "                print(f'no file found at {lvm_path}')\n",
    "                raise                \n",
    "        else:\n",
    "            print(f'invalid path {lvm_path}, must be pathlib.Path object')\n",
    "            raise TypeError\n",
    "        \n",
    "        # samples per bit\n",
    "        try:\n",
    "            assert isinstance(spb, int)\n",
    "            self.spb = spb\n",
    "            print(f'\\tset samples per bit: {self.spb}')\n",
    "        except AssertionError:\n",
    "            try:\n",
    "                self.spb = int(spb)\n",
    "                print(f'\\tset samples per bit: {self.spb}')\n",
    "            except Exception:\n",
    "                print(f'unable to convert provided spb ({type(spb)}) to integer, please provide in integer format')\n",
    "                raise\n",
    "                \n",
    "        # header\n",
    "        if header is None:\n",
    "            self.header = np.array([0 if i % 2 == 0 else 1 for i in range(10)])\n",
    "            print(f'\\tset header: {self.header}')\n",
    "        else:\n",
    "            if isinstance(header, list):\n",
    "                self.header = np.array(header)\n",
    "                print(f'\\tset header: {self.header}')\n",
    "            elif isinstance(header, np.array):\n",
    "                self.header = header\n",
    "                print(f'\\tset header: {self.header}')\n",
    "            else:\n",
    "                print(f'invalid header format specified ({type(header)}), must be np.array or list')\n",
    "                raise TypeError\n",
    "                \n",
    "        # startup tasks\n",
    "        self.parse_lvm_file(output_path='auto')\n",
    "        self.read_csv_data()\n",
    "        self.preprocess_data()\n",
    "        \n",
    "            \n",
    "    def parse_lvm_file(self, output_path='auto'):\n",
    "        \"\"\"\n",
    "        reads in .lvm file, writes out .csv with column names\n",
    "        containing only data points\n",
    "        \n",
    "        [arguments]\n",
    "        input_path: str or pathlib.Path object (preferred)\n",
    "            input file path\n",
    "        output_path: 'auto' (preferred) or str or pathlib.Path object\n",
    "            if auto:\n",
    "                appends _parsed.csv to end of input file name\n",
    "            else:\n",
    "                use provided path, must end in .csv\n",
    "                \n",
    "        [returns]\n",
    "        None\n",
    "            outputs parsed lvm file\n",
    "            sets self.parsed_lvm_path, pathlib.Path object pointing to parsed file\n",
    "        \"\"\"\n",
    "        \n",
    "        print('parsing .lvm file..')\n",
    "        with open(self.lvm_path, 'r') as infile:\n",
    "            # set output path\n",
    "            file_name = infile.name\n",
    "            \n",
    "            if output_path == 'auto':\n",
    "                self.parsed_lvm_path = Path(f'{file_name}_parsed.csv')\n",
    "            else:\n",
    "                self.parsed_lvm_path = output_path\n",
    "                \n",
    "                if isinstance(parsed_lvm_path, Path):\n",
    "                    assert str(parsed_lvm_path).endswith('.csv'), 'output file must end with .csv'\n",
    "                elif isinstance(parsed_lvm_path, str):\n",
    "                    assert parsed_lvm_path.endswith('.csv'), 'output file must end with .csv'\n",
    "                else:\n",
    "                    print('error, output_path must be str or pathlib.Path object')\n",
    "                    raise TypeError\n",
    "            \n",
    "            if not self.parsed_lvm_path.exists():\n",
    "                self.parsed_lvm_path.touch()\n",
    "            \n",
    "            # find header, store column names\n",
    "            while True:\n",
    "                line = infile.readline()\n",
    "                \n",
    "                if line.strip().endswith('Comment'):\n",
    "                    cols = line.split(',')[:-1]\n",
    "                    print(f'\\tcolumns: {cols}')\n",
    "                    break        \n",
    "                    \n",
    "            # trim file and write out\n",
    "            with open(self.parsed_lvm_path, 'w') as outfile:\n",
    "                outfile.write(', '.join(cols) + '\\n')\n",
    "                while True:\n",
    "                    try:\n",
    "                        outfile.write(next(infile))\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "        \n",
    "        print(f'\\tcomplete, parsed file saved to: {self.parsed_lvm_path}')\n",
    "        \n",
    "    def read_csv_data(self):\n",
    "        print('importing data from parsed csv..')\n",
    "        self.raw_data = np.genfromtxt(\n",
    "            self.parsed_lvm_path,\n",
    "            delimiter=',',\n",
    "            skip_header=1,\n",
    "            usecols=(1)\n",
    "        )\n",
    "        \n",
    "        print(f'\\timported data, {self.raw_data.size} rows')\n",
    "        \n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        th = max(self.raw_data) / 2\n",
    "        shape_check = self.raw_data[np.where(self.raw_data >= th)].size\n",
    "        \n",
    "        self.raw_data[np.where(self.raw_data >= th)] = 1\n",
    "        self.raw_data[np.where(self.raw_data < th)] = 0\n",
    "        \n",
    "        # set dtype and store\n",
    "        self.data = self.raw_data.astype('int')\n",
    "        \n",
    "        # validate\n",
    "        try:\n",
    "            assert self.data.sum() == shape_check            \n",
    "        except AssertionError:\n",
    "            print(f'error: sum of 1\\'s ({self.data.sum()}) does not match number of entries >= th ({shape_check})')\n",
    "        \n",
    "        \n",
    "    def get_state_length_list(self, data):\n",
    "        '''\n",
    "        takes data list ([1,1,0,1,0,0,...]) and returns numpy array \n",
    "        of duration of consecutive bits ([13,245,2588,19,1056,...])\n",
    "        '''\n",
    "        \n",
    "        return np.diff(\n",
    "            np.where(\n",
    "                np.concatenate(\n",
    "                    ([data[0]],\n",
    "                     data[:-1] != data[1:],\n",
    "                     [0]\n",
    "                    )\n",
    "                )\n",
    "            )[0]\n",
    "        )[::2]\n",
    "    \n",
    "\n",
    "    def discretize_signal(self):\n",
    "        # set params\n",
    "        discretized_data = []\n",
    "        start_bit = self.data[0]\n",
    "        alt_bit = 0 if start_bit == 1 else 1\n",
    "        print(f'starting bit: {start_bit}, alt bit: {alt_bit}')\n",
    "        \n",
    "        # get state lengths\n",
    "        state_lengths = self.get_state_length_list(self.data)\n",
    "        \n",
    "        for i, state in enumerate(state_lengths):\n",
    "            discretized_state = int(np.round(state / self.spb))\n",
    "            #print(f'iter: {i}, discretized state: {discretized_state}')\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                discretized_data.append([start_bit for j in range(discretized_state)])\n",
    "            elif i % 2 == 1:\n",
    "                discretized_data.append([alt_bit for j in range(discretized_state)])\n",
    "            else:\n",
    "                print('ya done messed up.')\n",
    "        \n",
    "        self.discretized_array = np.array([item for sublist in discretized_data for item in sublist])\n",
    "        print(f'complete, discretized signal: {self.discretized_array.size} entries')\n",
    "        \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing..\n",
      "\tset input lvm path: data\\qim_20kbps_10db_l2_v2.lvm\n",
      "\tset samples per bit: 20\n",
      "\tset header: [0 1 0 1 0 1 0 1 0 1]\n",
      "parsing .lvm file..\n",
      "columns: ['X_Value', 'Voltage']\n",
      "\tcomplete, parsed file saved to: data\\qim_20kbps_10db_l2_v2.lvm_parsed.csv\n",
      "\timported data, 4000000 rows\n"
     ]
    }
   ],
   "source": [
    "DP = DataParserLVM(lvm_path, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting bit: 0, alt bit: 1\n",
      "complete, discretized signal: 68851 entries\n"
     ]
    }
   ],
   "source": [
    "DP.discretize_signal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DP.discretized_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new lvm parser dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lvm_file(input_path, output_path='auto'):\n",
    "    \"\"\"\n",
    "    reads in .lvm file, writes out .csv with column names\n",
    "    containing only data points\n",
    "    \n",
    "    [arguments]\n",
    "    input_path: str or pathlib.Path object (preferred)\n",
    "        input file path\n",
    "    output_path: 'auto' (preferred) or str or pathlib.Path object\n",
    "        if auto:\n",
    "            appends _parsed.csv to end of input file name\n",
    "        else:\n",
    "            use provided path, must end in .csv\n",
    "            \n",
    "    [returns]\n",
    "    output_path: pathlib.Path object\n",
    "        outputs Path object pointing to outputted file   \n",
    "    \"\"\"\n",
    "\n",
    "    with open(lvm_path, 'r') as infile:\n",
    "        # set output path\n",
    "        file_name = infile.name\n",
    "        \n",
    "        if output_path == 'auto':\n",
    "            parsed_lvm_path = Path(f'{file_name}_parsed.csv')\n",
    "        else:\n",
    "            parsed_lvm_path = output_path\n",
    "            \n",
    "            if isinstance(parsed_lvm_path, pathlib.Path):\n",
    "                assert str(parsed_lvm_path).endswith('.csv'), 'output file must end with .csv'\n",
    "            elif isinstance(parsed_lvm_path, str):\n",
    "                assert parsed_lvm_path.endswith('.csv'), 'output file must end with .csv'\n",
    "            else:\n",
    "                print('error, output_path must be str or pathlib.Path object')\n",
    "        \n",
    "        if not parsed_lvm_path.exists():\n",
    "            parsed_lvm_path.touch()\n",
    "        \n",
    "        # find header, store column names\n",
    "        while True:\n",
    "            line = infile.readline()\n",
    "            \n",
    "            if line.strip().endswith('Comment'):\n",
    "                cols = line.split(',')[:-1]\n",
    "                print(f'columns: {cols}')\n",
    "                break        \n",
    "                \n",
    "        # trim file and write out\n",
    "        with open(parsed_lvm_path, 'w') as outfile:\n",
    "            outfile.write(', '.join(cols) + '\\n')\n",
    "            while True:\n",
    "                try:\n",
    "                    outfile.write(next(infile))\n",
    "                except StopIteration:\n",
    "                    break\n",
    "    \n",
    "    print(f'complete, parsed file saved to: {parsed_lvm_path}')\n",
    "    \n",
    "    return parsed_lvm_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_lvm_path = parse_lvm_file(lvm_path, output_path='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.genfromtxt(\n",
    "    parsed_lvm_path,\n",
    "    delimiter=',',\n",
    "    skip_header=1,\n",
    "    usecols=(1)\n",
    ")\n",
    "\n",
    "print(f'imported data, {raw_data.size} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set `1`'s and `0`'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = max(raw_data) / 2\n",
    "\n",
    "shape_check = raw_data[np.where(raw_data >= th)].size\n",
    "raw_data[np.where(raw_data >= th)] = 1\n",
    "raw_data[np.where(raw_data < th)] = 0\n",
    "\n",
    "# set dtype\n",
    "raw_data = raw_data.astype('int')\n",
    "\n",
    "# validate\n",
    "# sum of all the ones should equal the number of entries >= th\n",
    "assert raw_data.sum() == shape_check, 'error: sum of 1\\'s does not match number of entries >= th'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### discretize signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_length_list(data):\n",
    "    '''\n",
    "    takes data list ([1,1,0,1,0,0,...]) and returns numpy array \n",
    "    of duration of consecutive bits ([13,245,2588,19,1056,...])\n",
    "    '''\n",
    "    \n",
    "    return np.diff(\n",
    "        np.where(\n",
    "            np.concatenate(\n",
    "                ([data[0]],\n",
    "                 data[:-1] != data[1:],\n",
    "                 [0]\n",
    "                )\n",
    "            )\n",
    "        )[0]\n",
    "    )[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49, 19,  9, 10, 30, 17,  2, 49, 49, 19], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_lengths = get_state_length_list(DP.raw_data)\n",
    "state_lengths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_signal(raw_data, spb):\n",
    "    discretized_data = []\n",
    "    start_bit = raw_data[0]\n",
    "    alt_bit = 0 if start_bit == 1 else 1\n",
    "    #print(f'starting bit: {start_bit}, alt bit: {alt_bit}')\n",
    "    \n",
    "    \n",
    "    for i, state in enumerate(state_lengths):\n",
    "        discretized_state = int(np.round(state / spb))\n",
    "        #print(f'iter: {i}, discretized state: {discretized_state}')\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            # start_bit\n",
    "            discretized_data.append([start_bit for j in range(discretized_state)])\n",
    "        elif i % 2 == 1:\n",
    "            # alt_bit\n",
    "            discretized_data.append([alt_bit for j in range(discretized_state)])\n",
    "        else:\n",
    "            print('ya done messed up.')\n",
    "    \n",
    "    discretized_array = np.array([item for sublist in discretized_data for item in sublist])\n",
    "    #print(f'complete, discretized signal: {discretized_array.size} entries')\n",
    "    \n",
    "    return discretized_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretized_array = discretize_signal(raw_data, 92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretized_array[95:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### signal finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_sequence_numpy(arr,seq):\n",
    "    \"\"\" Find sequence in an array using NumPy only.\n",
    "\n",
    "    Parameters\n",
    "    ----------    \n",
    "    arr    : input 1D array\n",
    "    seq    : input 1D array\n",
    "\n",
    "    Output\n",
    "    ------    \n",
    "    Output : 1D Array of indices in the input array that satisfy the \n",
    "    matching of input sequence in the input array.\n",
    "    In case of no match, an empty list is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    # Store sizes of input array and sequence\n",
    "    Na, Nseq = arr.size, seq.size\n",
    "\n",
    "    # Range of sequence\n",
    "    r_seq = np.arange(Nseq)\n",
    "\n",
    "    # Create a 2D array of sliding indices across the entire length of input array.\n",
    "    # Match up with the input sequence & get the matching starting indices.\n",
    "    M = (arr[np.arange(Na-Nseq+1)[:,None] + r_seq] == seq).all(1)\n",
    "\n",
    "    # Get the range of those indices as final output\n",
    "    if M.any() >0:\n",
    "        return np.where(np.convolve(M,np.ones((Nseq),dtype=int))>0)[0]\n",
    "    else:\n",
    "        return []  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sequence_numpy(discretized_array, np.array(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretized_array[30078:30120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spb brute force attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spb_list: 10 elements\n"
     ]
    }
   ],
   "source": [
    "spb_list = [i for i in range(90, 100, 1)]\n",
    "print(f'spb_list: {len(spb_list)} elements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9351a3d54547a49e8ddd616848ca39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for spb in tqdm_notebook(spb_list):\n",
    "    discretized_array = discretize_signal(DP.raw_data, spb)    \n",
    "    \n",
    "    result_dict[spb] = search_sequence_numpy(discretized_array, np.array(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 807), (191, 38), (192, 21)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies = {\n",
    "    spb: Counter(np.diff(matches)).most_common(3)\n",
    "    for spb, matches in result_dict.items()\n",
    "}\n",
    "\n",
    "frequencies[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{90: [(1, 807), (191, 38), (192, 21)],\n",
       " 91: [(1, 807), (191, 38), (192, 21)],\n",
       " 92: [(1, 807), (190, 69), (191, 2)],\n",
       " 93: [(1, 807), (190, 69), (191, 2)],\n",
       " 94: [(1, 798), (189, 37), (188, 18)],\n",
       " 95: [(1, 798), (189, 37), (188, 18)],\n",
       " 96: [(1, 783), (188, 56), (187, 8)],\n",
       " 97: [(1, 783), (188, 56), (187, 8)],\n",
       " 98: [(1, 397), (41, 2), (98, 1)],\n",
       " 99: [(1, 397), (41, 2), (98, 1)]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
