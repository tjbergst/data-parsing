{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# translating matlab code parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid',rc={'figure.facecolor':'#abb2bf'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "#lvm_path = Path(r'data/qim_20kbps_10db_l2_v2.lvm')\n",
    "lvm_path = Path(r'data/test_packet.lvm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParserLVM:\n",
    "    def __init__(self, lvm_path, source='txt', header=None, preprocess_mode='static'):\n",
    "        print('initializing..')\n",
    "        \n",
    "        # set input path\n",
    "        if isinstance(lvm_path, Path):\n",
    "            try: \n",
    "                assert lvm_path.exists()\n",
    "                self.lvm_path = lvm_path\n",
    "                print(f'\\tset input lvm path: {self.lvm_path}')\n",
    "            except AssertionError:\n",
    "                print(f'no file found at {lvm_path}')\n",
    "                raise                \n",
    "        else:\n",
    "            print(f'invalid path {lvm_path}, must be pathlib.Path object')\n",
    "            raise TypeError\n",
    "                \n",
    "        # header\n",
    "        if header is None:\n",
    "            #self.header = np.array([1 if i % 2 == 0 else 0 for i in range(30)])\n",
    "            self.header = np.ones(50, dtype=int)\n",
    "            print(f'\\tset header: {self.header}')\n",
    "        else:\n",
    "            if isinstance(header, list):\n",
    "                self.header = np.array(header)\n",
    "                print(f'\\tset header: {self.header}')\n",
    "            elif isinstance(header, np.ndarray):\n",
    "                self.header = header\n",
    "                print(f'\\tset header: {self.header}')\n",
    "            else:\n",
    "                print(\n",
    "                    f'invalid header format specified ({type(header)}), ' + \n",
    "                    f'must be np.array or list'\n",
    "                )\n",
    "                raise TypeError\n",
    "        \n",
    "        # source\n",
    "        if source == 'txt':\n",
    "            self.parse_other_file()\n",
    "        elif source == 'lvm':\n",
    "            self.parse_lvm_file()\n",
    "            self.read_csv_data()\n",
    "        else:\n",
    "            print(\n",
    "                f'invalid source {source}, options are ' + \n",
    "                '\\'txt\\' for tab delimited numbers only or ' + \n",
    "                '\\'lvm\\' for lvm file with headers'\n",
    "            )\n",
    "            \n",
    "        # data message\n",
    "        self.data_msg = self.create_data_message()\n",
    "        print(f'created data message, {self.data_msg.size} bits')\n",
    "        \n",
    "        # startup tasks\n",
    "        self.preprocess_data(mode=preprocess_mode)\n",
    "        \n",
    "    #------------------------------ ingestion and processing ------------------------------\n",
    "    def create_data_message(self):\n",
    "        scaler = 1\n",
    "        base = np.array([1 if i % 2 == 0 else 0 for i in range(20)])\n",
    "        \n",
    "        return np.concatenate((\n",
    "            np.repeat(base, scaler), \n",
    "            np.repeat(base, scaler), \n",
    "            np.repeat(base, 2*scaler), \n",
    "            np.repeat(base, scaler), \n",
    "            np.repeat(base, scaler), \n",
    "            np.repeat(base, 2*scaler)\n",
    "        ))\n",
    "        \n",
    "        \n",
    "    def parse_other_file(self):\n",
    "        print('parsing tab delimited file..')\n",
    "        self.raw_data = np.genfromtxt(\n",
    "            self.lvm_path,\n",
    "            delimiter='\\t',\n",
    "            usecols=(1)\n",
    "        )        \n",
    "        print(f'\\timported data, {self.raw_data.size} rows')\n",
    "        \n",
    "            \n",
    "    def parse_lvm_file(self, output_path='auto'):\n",
    "        \"\"\"\n",
    "        reads in .lvm file, writes out .csv with column names\n",
    "        containing only data points\n",
    "        \n",
    "        [arguments]\n",
    "        input_path: str or pathlib.Path object (preferred)\n",
    "            input file path\n",
    "        output_path: 'auto' (preferred) or str or pathlib.Path object\n",
    "            if auto:\n",
    "                appends _parsed.csv to end of input file name\n",
    "            else:\n",
    "                use provided path, must end in .csv\n",
    "                \n",
    "        [returns]\n",
    "        None\n",
    "            outputs parsed lvm file\n",
    "            sets self.parsed_lvm_path, pathlib.Path object pointing to parsed file\n",
    "        \"\"\"\n",
    "        \n",
    "        print('parsing lvm file..')\n",
    "        with open(self.lvm_path, 'r') as infile:\n",
    "            # set output path\n",
    "            file_name = infile.name\n",
    "            \n",
    "            if output_path == 'auto':\n",
    "                self.parsed_lvm_path = Path(f'{file_name}_parsed.csv')\n",
    "            else:\n",
    "                self.parsed_lvm_path = output_path\n",
    "                \n",
    "                if isinstance(parsed_lvm_path, Path):\n",
    "                    try:\n",
    "                        assert str(parsed_lvm_path).endswith('.csv')\n",
    "                    except AssertionError:\n",
    "                        print('output file must end with .csv')\n",
    "                        raise\n",
    "                elif isinstance(parsed_lvm_path, str):\n",
    "                    try:\n",
    "                        assert parsed_lvm_path.endswith('.csv')\n",
    "                    except AssertionError:\n",
    "                        print('output file must end with .csv')\n",
    "                        raise\n",
    "                else:\n",
    "                    print('error, output_path must be str or pathlib.Path object')\n",
    "                    raise TypeError\n",
    "            \n",
    "            if not self.parsed_lvm_path.exists():\n",
    "                self.parsed_lvm_path.touch()\n",
    "            \n",
    "            # find header, store column names\n",
    "            while True:\n",
    "                line = infile.readline()\n",
    "                \n",
    "                if line.strip().endswith('Comment'):\n",
    "                    cols = line.split(',')[:-1]\n",
    "                    print(f'\\tcolumns: {cols}')\n",
    "                    break        \n",
    "                    \n",
    "            # trim file and write out\n",
    "            with open(self.parsed_lvm_path, 'w') as outfile:\n",
    "                outfile.write(', '.join(cols) + '\\n')\n",
    "                while True:\n",
    "                    try:\n",
    "                        outfile.write(next(infile))\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "        \n",
    "        print(f'\\tcomplete, parsed file saved to: {self.parsed_lvm_path}')\n",
    "        \n",
    "    def read_csv_data(self):\n",
    "        print('importing data from parsed csv..')\n",
    "        self.raw_data = np.genfromtxt(\n",
    "            self.parsed_lvm_path,\n",
    "            delimiter=',',\n",
    "            skip_header=1,\n",
    "            usecols=(1)\n",
    "        )\n",
    "        \n",
    "        print(f'\\timported data, {self.raw_data.size} rows')\n",
    "        \n",
    "        \n",
    "    def preprocess_data(self, mode='static'):\n",
    "        \"\"\"\n",
    "        convert float data into 1's and 0's\n",
    "        \n",
    "        static: use 1/2 * max data point as threshold\n",
    "        dynamic: use min + 1/2(max - min) as threshold\n",
    "        \"\"\"\n",
    "        \n",
    "        if mode == 'static':\n",
    "            th = self.raw_data.max() / 2\n",
    "        elif mode == 'dynamic':\n",
    "            th = self.raw_data.min() + (self.raw_data.max() - self.raw_data.min()) / 2\n",
    "        else:\n",
    "            print(f'invalid mode {mode}, options are \\'static\\' or \\'dynamic\\'')        \n",
    "        \n",
    "        shape_check = self.raw_data[np.where(self.raw_data >= th)].size\n",
    "        \n",
    "        self.raw_data[np.where(self.raw_data >= th)] = 1\n",
    "        self.raw_data[np.where(self.raw_data < th)] = 0\n",
    "        \n",
    "        # set dtype and store\n",
    "        self.data = self.raw_data.astype('int')[7:]\n",
    "        \n",
    "        # validate\n",
    "        try:\n",
    "            assert self.data.sum() == shape_check            \n",
    "        except AssertionError:\n",
    "            print(\n",
    "                f'error: sum of 1\\'s ({self.data.sum()}) does not match ' + \n",
    "                f'number of entries >= th ({shape_check})'\n",
    "            )\n",
    "        \n",
    "    #---------------------------------- discretization methods ----------------------------------    \n",
    "    def get_state_length_list(self, data):\n",
    "        '''\n",
    "        takes data list ([1,1,0,1,0,0,...]) and returns numpy array \n",
    "        of duration of consecutive bits ([13,245,2588,19,1056,...])\n",
    "        '''\n",
    "        \n",
    "        return np.diff(\n",
    "            np.where(\n",
    "                np.concatenate(\n",
    "                    ([data[0]],\n",
    "                     data[:-1] != data[1:],\n",
    "                     [0]\n",
    "                    )\n",
    "                )\n",
    "            )[0]\n",
    "        )[::2]\n",
    "    \n",
    "    def get_correct_state_length_list(self, data):\n",
    "        '''\n",
    "        gets state lengths like above, but correctly..\n",
    "        '''\n",
    "        \n",
    "        groups = groupby(data)\n",
    "        \n",
    "        return [sum(1 for _ in group) for _, group in groups]\n",
    "    \n",
    "\n",
    "    def discretize_signal(self, spb=12):\n",
    "        start_time = time.time()\n",
    "        print('discretizing signal..')\n",
    "        \n",
    "        # set params\n",
    "        self.spb = spb\n",
    "        \n",
    "        discretized_data = []\n",
    "        start_bit = self.data[0]\n",
    "        alt_bit = 0 if start_bit == 1 else 1\n",
    "        print(f'\\tstarting bit: {start_bit}, alt bit: {alt_bit}')\n",
    "        \n",
    "        # get state lengths\n",
    "        #state_lengths = self.get_state_length_list(self.data)\n",
    "        state_lengths = self.get_correct_state_length_list(self.data)\n",
    "        \n",
    "        for i, state in enumerate(state_lengths):\n",
    "            discretized_state = int(np.round(state / self.spb))\n",
    "            #print(f'iter: {i}, discretized state: {discretized_state}')\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                discretized_data.append([start_bit for j in range(discretized_state)])\n",
    "            elif i % 2 == 1:\n",
    "                discretized_data.append([alt_bit for j in range(discretized_state)])\n",
    "            else:\n",
    "                print('ya done messed up.')\n",
    "        \n",
    "        self.discretized_array = np.array([\n",
    "            item for sublist \n",
    "                in discretized_data \n",
    "            for item in sublist\n",
    "        ])\n",
    "        \n",
    "        reduction = 100 * (self.discretized_array.size / self.data.size)\n",
    "        print(\n",
    "            f'\\tcomplete, discretized signal: {self.discretized_array.size} entries ' + \n",
    "            f'({reduction:0.2f}% of original)\\n'\n",
    "            f'\\tprocessing time: {time.time() - start_time:0.4f} sec'\n",
    "        )\n",
    "        \n",
    "    #---------------------------------- search and ber ----------------------------------\n",
    "    def search_sequence_numpy(self, arr, seq):\n",
    "        \"\"\" \n",
    "        find sequence in an array \n",
    "    \n",
    "        Parameters\n",
    "        ----------    \n",
    "        arr    : input 1D array\n",
    "        seq    : input 1D array\n",
    "    \n",
    "        Output\n",
    "        ------    \n",
    "        Output : array of starting indices of matches\n",
    "        \"\"\"\n",
    "        \n",
    "        # validate inputs\n",
    "        try:\n",
    "            assert isinstance(arr, np.ndarray)\n",
    "        except AssertionError:\n",
    "            print(f'input arr must be numpy array, you provided {type(arr)}')\n",
    "            raise\n",
    "            \n",
    "        try:\n",
    "            assert isinstance(seq, np.ndarray)\n",
    "        except AssertionError:\n",
    "            print(f'input seq must be numpy array, you provided {type(arr)}')\n",
    "            raise\n",
    "    \n",
    "        # Store sizes of input array and sequence\n",
    "        Na, Nseq = arr.size, seq.size\n",
    "    \n",
    "        # Range of sequence\n",
    "        r_seq = np.arange(Nseq)\n",
    "    \n",
    "        # Create a 2D array of sliding indices across the entire length of input array.\n",
    "        # Match up with the input sequence & get the matching starting indices.\n",
    "        M = (arr[np.arange(Na-Nseq+1)[:, None] + r_seq] == seq).all(1)\n",
    "    \n",
    "        # Get the range of those indices as final output\n",
    "        if M.any() > 0:\n",
    "            tmp_array =  np.where(\n",
    "                np.convolve(\n",
    "                    M,\n",
    "                    np.ones((Nseq), dtype=int)\n",
    "                ) > 0\n",
    "            )[0]\n",
    "        else:\n",
    "            tmp_array =  []\n",
    "        \n",
    "        \n",
    "        match_starting_idxs = [\n",
    "            list(map(itemgetter(1), g))[0]\n",
    "            for k, g in groupby(enumerate(tmp_array), lambda ix: ix[0] - ix[1])\n",
    "        ]\n",
    "        \n",
    "        print(f'found {len(match_starting_idxs)} matches')\n",
    "        \n",
    "        return match_starting_idxs\n",
    "        \n",
    "        \n",
    "    def get_signal_stats(self, data):\n",
    "        state_lengths = self.get_correct_state_length_list(data)\n",
    "        \n",
    "        return Counter(state_lengths).most_common()\n",
    "    \n",
    "    \n",
    "    def compute_bit_error_rate(self):\n",
    "        # find headers\n",
    "        self.header_starts = self.search_sequence_numpy(self.discretized_array, self.header)\n",
    "        #print(f'found {len(self.header_starts)} headers')\n",
    "        \n",
    "        # slice data packets\n",
    "        self.ber = []\n",
    "        clipped_packets = 0\n",
    "        \n",
    "        for header in self.header_starts:\n",
    "            start = header + len(self.header) + 4\n",
    "            stop = start + len(self.data_msg)\n",
    "            \n",
    "            if self.discretized_array[start:stop].size == 160:\n",
    "                data_slice = self.discretized_array[start:stop]\n",
    "                self.ber.append((data_slice - self.data_msg).sum())\n",
    "            else:\n",
    "                clipped_packets += 1\n",
    "                \n",
    "        print(f'complete, found {clipped_packets} clipped packet(s)')\n",
    "        print(\n",
    "            f'bit error breakdown:\\n' + \n",
    "            f'\\ttotal messages computed: {len(self.ber)}\\n' + \n",
    "            f'\\tcumulative bit errors: {sum(self.ber)}\\n' + \n",
    "            f'\\tmean bit errors: {sum(self.ber) / len(self.ber)}\\n' + \n",
    "            f'\\tprobably all zero, if not all bit errors are stored in self.ber'\n",
    "        )\n",
    "        \n",
    "        \n",
    "    #---------------------------------- output methods ----------------------------------\n",
    "    def output_discretized_array(self, output_path='auto', output_format='csv'):\n",
    "        # set output path\n",
    "        if output_path == 'auto':\n",
    "            output_path = Path(r'data/discretized_array.csv')            \n",
    "        elif isinstance(output_path, Path):\n",
    "            try:\n",
    "                assert output_path.parent.exists()\n",
    "                assert output_path.name.endswith('.csv')\n",
    "            except AssertionError:\n",
    "                print(\n",
    "                    f'parent path ({output_path.parent}) unavailable or ' + \n",
    "                    f'path does not end with .csv ({output_path.name}), ' + \n",
    "                    f'please choose another path'\n",
    "                )\n",
    "                raise\n",
    "        elif isinstance(output_path, str):\n",
    "            try:\n",
    "                output_path = Path(output_path)\n",
    "                assert output_path.parent.exists()\n",
    "                assert output_path.name.endswith('.csv')\n",
    "            except AssertionError:\n",
    "                print(\n",
    "                    f'parent path ({output_path.parent}) unavailable or ' + \n",
    "                    f'path does not end with .csv ({output_path.name}), ' + \n",
    "                    f'please choose another path'\n",
    "                )\n",
    "                raise\n",
    "        else:\n",
    "            print(\n",
    "                f'you really messed up, you provided {output_path} {type(output_path)}, ' + \n",
    "                f'please provide output_path as str or pathlib.Path object ending with .csv'\n",
    "            )            \n",
    "        print(f'set discretized signal output path: {output_path}')\n",
    "                \n",
    "        # output\n",
    "        if output_format == 'csv':\n",
    "            np.savetxt(\n",
    "                output_path,\n",
    "                self.discretized_array,\n",
    "                delimiter=','\n",
    "            )\n",
    "            print(f'\\tsuccessfully saved file to: {output_path}')\n",
    "        else:\n",
    "            print(f'yeah.. csv is actually the only option.. please try again.')\n",
    "            raise TypeError\n",
    "            \n",
    "    \n",
    "    #---------------------------------- plotting methods ----------------------------------\n",
    "    \n",
    "    def plot_subset(self, data, start, stop):\n",
    "        plt.figure(figsize=(20, 6))\n",
    "        \n",
    "        plt.stem(data[start:stop])\n",
    "    \n",
    "    def compare_plots(self, start, stop, title=None):\n",
    "        if title is None:\n",
    "            title='no title provided'\n",
    "        \n",
    "        fig, ((ax1, ax2)) = plt.subplots(2, 1, sharex=False, sharey=True, figsize=(15,6))\n",
    "        \n",
    "        ax1.stem(DP.data[start*DP.spb:stop*DP.spb])\n",
    "        ax2.stem(DP.discretized_array[start:stop]) \n",
    "        \n",
    "        ax1.set_title(title)\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing..\n",
      "\tset input lvm path: data\\test_packet.lvm\n",
      "\tset header: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "parsing tab delimited file..\n",
      "\timported data, 3500000 rows\n",
      "created data message, 160 bits\n"
     ]
    }
   ],
   "source": [
    "DP = DataParserLVM(\n",
    "    lvm_path, \n",
    "    source='txt',\n",
    "    preprocess_mode='dynamic'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discretizing signal..\n",
      "\tstarting bit: 1, alt bit: 0\n",
      "\tcomplete, discretized signal: 291668 entries (8.33% of original)\n",
      "\tprocessing time: 1.5550 sec\n"
     ]
    }
   ],
   "source": [
    "DP.discretize_signal(spb=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1363 matches\n",
      "found 1363 headers\n",
      "complete, found 1 clipped packet(s)\n",
      "bit error breakdown:\n",
      "\ttotal messages computed: 1362\n",
      "\tcumulative bit errors: 0\n",
      "\tmean bit errors: 0.0\n",
      "\tprobably all zero, if not all bit errors are stored in self.ber\n"
     ]
    }
   ],
   "source": [
    "DP.compute_bit_error_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DP.plot_subset(\n",
    "    DP.discretized_array, \n",
    "    start=DP.header_starts[0], \n",
    "    stop=DP.header_starts[0] + len(DP.header) + len(DP.data_msg) + 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DP.plot_subset(\n",
    "    DP.discretized_array, \n",
    "    start=DP.header_starts[25], \n",
    "    stop=DP.header_starts[25] + len(DP.header) + len(DP.data_msg) + 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### signal stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14, 39780),\n",
       " (10, 39722),\n",
       " (22, 19880),\n",
       " (26, 19877),\n",
       " (11, 14791),\n",
       " (13, 14733),\n",
       " (25, 7383),\n",
       " (23, 7380),\n",
       " (599, 1273),\n",
       " (49, 1190),\n",
       " (48, 173),\n",
       " (600, 90),\n",
       " (5, 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DP.get_signal_stats(DP.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
